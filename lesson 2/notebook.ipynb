{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2\n",
    "\n",
    "### Описание\n",
    "\n",
    "В вашем распоряжении датасет с русскоязычными отзывами о мобильных телефонах с выставленным рейтингом от 1 до 5.\n",
    "Ключевая задача – обучить любую модель регрессии (или классификации, если решите таким путём пойти) из пакетов scikit, XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "\n",
    "Необходимая метрика:\n",
    "\n",
    "1. Со звёздочкой (дополнительный балл) – MAE <= 0.5\n",
    "2. Минимальное допустимое значение – МАЕ <= 1.0\n",
    "\n",
    "### Что необходимо сделать\n",
    "\n",
    "1. Откройте датасет\n",
    "2. Разделите на обучение и тест\n",
    "3. Осуществите лемматизацию с помощью любого из озвученных на занятии инструментов \n",
    "4. Обучение одну или несколько моделей машинного обучения на разных представлениях данных\n",
    "5. Валидируйте модель. Если модель соответствует условиям метрик, то работа завершена. В ином случае, экспериментируйте, начиная с пункта 7. \n",
    "6. По всем попыткам обучить качественную модель пишите свои выводы и замечания, почему так получилось.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0. Импорт библиотк, определение констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3d touch просто восхитительная вещь  заряд дер...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3d touch просто восхитительный вещь заряд держ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>отключается при температуре близкой к нулю  не...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>отключаться температура близкий нуль непонятно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>в apple окончательно решили не заморачиваться ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>apple окончательно решить не заморачиваться де...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>постарался наиболее ёмко и коротко описать все...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>постараться наиболее ёмко коротко описать всё ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>достойный телефон  пользоваться одно удовольст...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>достойный телефон пользоваться удовольствие</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  \\\n",
       "0  3d touch просто восхитительная вещь  заряд дер...     5.0   \n",
       "1  отключается при температуре близкой к нулю  не...     4.0   \n",
       "2  в apple окончательно решили не заморачиваться ...     3.0   \n",
       "3  постарался наиболее ёмко и коротко описать все...     4.0   \n",
       "4  достойный телефон  пользоваться одно удовольст...     5.0   \n",
       "\n",
       "                                               lemma  \n",
       "0  3d touch просто восхитительный вещь заряд держ...  \n",
       "1  отключаться температура близкий нуль непонятно...  \n",
       "2  apple окончательно решить не заморачиваться де...  \n",
       "3  постараться наиболее ёмко коротко описать всё ...  \n",
       "4        достойный телефон пользоваться удовольствие  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"data/data_lemma_cleared.csv\"):\n",
    "    df = pd.read_csv(\"data/data_lemma_cleared.csv\", engine='python')\n",
    "\n",
    "df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Review', axis=1)\n",
    "df = df.dropna()\n",
    "df.columns = ['label', 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оставлю только небольшой кусок данных на время разработки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14626103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d',\n",
       " 'touch',\n",
       " 'просто',\n",
       " 'восхитительный',\n",
       " 'вещь',\n",
       " 'заряд',\n",
       " 'держать',\n",
       " 'целый',\n",
       " 'день',\n",
       " 'розовый']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = df.review.values\n",
    "words = ' '.join(reviews)\n",
    "words = words.split()\n",
    "\n",
    "print(len(words))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "int2word[0] = '<PAD>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220502"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 319791/319791 [00:04<00:00, 77146.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1396, 1639, 19, 5984, 395]\n",
      "[1005, 2310, 813, 2561, 1349]\n",
      "[587, 1964, 132, 1, 2138]\n",
      "[1819, 2905, 31073, 4916, 788]\n",
      "[223, 2, 17, 641]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_enc = []\n",
    "\n",
    "for review in tqdm(reviews):\n",
    "    reviews_enc += [[]]\n",
    "    \n",
    "    for word in review.split():\n",
    "        reviews_enc[-1].append(word2int[word])\n",
    "\n",
    "for i in range(5):\n",
    "    print(reviews_enc[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319791, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    features = np.full((len(reviews), \n",
    "                        seq_length), \n",
    "                       pad_id, \n",
    "                       dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "seq_length = 256\n",
    "features = pad_features(reviews_enc, \n",
    "                        pad_id=word2int['<PAD>'], \n",
    "                        seq_length=seq_length)\n",
    "\n",
    "assert len(features) == len(reviews_enc)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 4., 3., ..., 5., 5., 5.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.label.to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "===============\n",
      "Train set: (223853, 256)\n",
      "Validation set: (47969, 256)\n",
      "Test set: (47969, 256)\n"
     ]
    }
   ],
   "source": [
    "train_size = .7\n",
    "val_size = .5\n",
    "\n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, remain_x = features[:split_id], features[split_id:]\n",
    "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "split_val_id = int(len(remain_x) * val_size)\n",
    "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
    "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n",
    "\n",
    "print('Feature Shapes:')\n",
    "print('===============')\n",
    "print('Train set: {}'.format(train_x.shape))\n",
    "print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 64\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), \n",
    "                         torch.from_numpy(train_y))\n",
    "\n",
    "\n",
    "validset = TensorDataset(torch.from_numpy(val_x), \n",
    "                         torch.from_numpy(val_y))\n",
    "\n",
    "testset = TensorDataset(torch.from_numpy(test_x), \n",
    "                        torch.from_numpy(test_y))\n",
    "\n",
    "train_iterator = DataLoader(trainset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "valid_iterator = DataLoader(validset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "test_iterator = DataLoader(testset, \n",
    "                           shuffle=True, \n",
    "                           batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция подсчета accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    error = torch.mean(torch.abs(rounded_preds - y).float())\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch[0].T.cuda()).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions.float(), \n",
    "                          batch[1].float().cuda())\n",
    "        \n",
    "        acc = mae(predictions.float(), \n",
    "                              batch[1].float().cuda())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0].T.cuda()).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions.float(), \n",
    "                              batch[1].float().cuda())\n",
    "            \n",
    "            acc = mae(predictions.float(), \n",
    "                                  batch[1].float().cuda())\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_dim, \n",
    "                 n_filters, \n",
    "                 filter_sizes, \n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embedding_dim)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[0], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[1], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[2], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, \n",
    "                            output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = [sent len, batch size]\n",
    "        x = x.permute(1, 0)\n",
    "\n",
    "        #x = [batch size, sent len]\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = len(word2int)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            N_FILTERS, \n",
    "            FILTER_SIZES, \n",
    "            OUTPUT_DIM, \n",
    "            DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(220502, 100)\n",
       "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.590, Train Acc: 53.01%, Val. Loss: 0.839, Val. Acc: 64.69%\n",
      "Epoch: 02, Train Loss: 0.538, Train Acc: 49.64%, Val. Loss: 0.841, Val. Acc: 64.30%\n",
      "Epoch: 03, Train Loss: 0.497, Train Acc: 46.69%, Val. Loss: 0.856, Val. Acc: 63.48%\n",
      "Epoch: 04, Train Loss: 0.461, Train Acc: 44.12%, Val. Loss: 0.885, Val. Acc: 67.24%\n",
      "Epoch: 05, Train Loss: 0.437, Train Acc: 42.05%, Val. Loss: 0.862, Val. Acc: 63.94%\n",
      "Epoch: 06, Train Loss: 0.416, Train Acc: 40.54%, Val. Loss: 0.858, Val. Acc: 63.84%\n",
      "Epoch: 07, Train Loss: 0.399, Train Acc: 39.03%, Val. Loss: 0.849, Val. Acc: 63.57%\n",
      "Epoch: 08, Train Loss: 0.384, Train Acc: 38.00%, Val. Loss: 0.869, Val. Acc: 63.31%\n",
      "Epoch: 09, Train Loss: 0.373, Train Acc: 36.99%, Val. Loss: 0.882, Val. Acc: 64.20%\n",
      "Epoch: 10, Train Loss: 0.361, Train Acc: 35.88%, Val. Loss: 0.869, Val. Acc: 63.12%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(model, \n",
    "                                       train_iterator, \n",
    "                                       optimizer, \n",
    "                                       criterion)\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate_func(model, \n",
    "                                          valid_iterator, \n",
    "                                          criterion)\n",
    "    \n",
    "    train_msg = f'Epoch: {epoch+1:02}, '\n",
    "    train_msg += f'Train Loss: {train_loss:.3f}, '\n",
    "    train_msg += f'Train Acc: {train_acc:.2f}, '\n",
    "    train_msg += f'Val. Loss: {valid_loss:.3f}, '\n",
    "    train_msg += f'Val. Acc: {valid_acc:.2f}'\n",
    "    \n",
    "    print(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.908, Test Acc: 0.67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss , test_acc = evaluate_func(model, \n",
    "                                     test_iterator, \n",
    "                                     criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch size:  torch.Size([64, 256])\n",
      "Sample batch input: \n",
      " tensor([[2419,   16,    2,  ...,    0,    0,    0],\n",
      "        [  41, 1713,   33,  ...,    0,    0,    0],\n",
      "        [ 349,   81,  876,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  62,   17,  108,  ...,    0,    0,    0],\n",
      "        [   2,   22,  783,  ...,    0,    0,    0],\n",
      "        [  36,   77,   11,  ...,    0,    0,    0]])\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label input: \n",
      " tensor([5., 3., 3., 1., 1., 5., 5., 4., 5., 5., 4., 5., 3., 5., 5., 5., 4., 5.,\n",
      "        5., 3., 5., 3., 4., 5., 1., 2., 2., 4., 1., 2., 2., 2., 5., 5., 4., 5.,\n",
      "        3., 5., 5., 5., 4., 3., 5., 5., 2., 4., 1., 5., 5., 5., 5., 4., 4., 4.,\n",
      "        5., 3., 4., 5., 4., 1., 5., 5., 5., 5.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_iterator)\n",
    "x, y = next(dataiter)\n",
    "\n",
    "print('Sample batch size: ', x.size()) \n",
    "print('Sample batch input: \\n', x)\n",
    "print()\n",
    "print('Sample label size: ', y.size())\n",
    "print('Sample label input: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size=128, \n",
    "                 embedding_size=400, n_layers=2, dropout=0.2):\n",
    "        \n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.fc2 = nn.Linear(embedding_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # convert feature to long\n",
    "        x = x.long()\n",
    "\n",
    "        # map input to vector\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #x = self.fc2(x)\n",
    "        \n",
    "        # pass forward to lstm\n",
    "        x, _ =  self.lstm(x)\n",
    "\n",
    "        # get last sequence output\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # apply dropout and fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # sigmoid\n",
    "        # o = self.sigmoid(o)\n",
    "        \n",
    "        # o = o * 5\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentModel(\n",
      "  (embedding): Embedding(220502, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2int)\n",
    "output_size = 1\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "dropout=0.25\n",
    "\n",
    "model = SentimentModel(vocab_size, \n",
    "                       output_size, \n",
    "                       hidden_size, \n",
    "                       embedding_size, \n",
    "                       n_layers, \n",
    "                       dropout)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "criterion = torch.nn.MSELoss()\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "grad_clip = 1\n",
    "epochs = 5\n",
    "print_every = 1\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'train_mae': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_mae': [],\n",
    "    'epochs': epochs\n",
    "}\n",
    "es_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [04:04<16:16, 244.20s/it, Val Loss: 50.485 | Val mae: 30.810]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 51.985 Train mae: 31.729 | Val Loss: 50.485 Val mae: 30.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 3/5 [08:14<05:37, 168.80s/it, Val Loss: 50.484 | Val mae: 30.809]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 50.583 Train mae: 31.467 | Val Loss: 50.484 Val mae: 30.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4/5 [12:36<03:25, 205.85s/it, Val Loss: 50.287 | Val mae: 30.806]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 50.655 Train mae: 31.481 | Val Loss: 50.287 Val mae: 30.806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 6it [16:54, 174.18s/it, Training batch 3/3498]                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 50.469 Train mae: 31.467 | Val Loss: 50.739 Val mae: 30.810\n",
      "[WARNING] Validation loss did not improved (50.287 --> 50.739)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [21:09<00:00, 253.96s/it, Val Loss: 50.405 | Val mae: 30.812]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 50.500 Train mae: 31.462 | Val Loss: 50.405 Val mae: 30.812\n",
      "[WARNING] Validation loss did not improved (50.287 --> 50.405)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "model = SentimentModel(vocab_size, \n",
    "                       output_size, \n",
    "                       hidden_size, \n",
    "                       embedding_size, \n",
    "                       n_layers, \n",
    "                       dropout)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "grad_clip = 1\n",
    "epochs = 5\n",
    "print_every = 1\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'train_mae': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_mae': [],\n",
    "    'epochs': epochs\n",
    "}\n",
    "es_limit = 5\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "\n",
    "# early stop trigger\n",
    "es_trigger = 0\n",
    "val_loss_min = 1000 #torch.inf\n",
    "\n",
    "for e in epochloop:\n",
    "\n",
    "    # Обучение\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_mae = 0\n",
    "    \n",
    "    for id_, (feature, target) in enumerate(train_iterator):\n",
    "        epochloop.set_postfix_str(f'Training batch {id_}/{len(train_iterator)}')\n",
    "\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        out = model(feature)\n",
    "        #print(out[:5])\n",
    "        #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        #predicted = torch.tensor(torch.round(out), device=device)\n",
    "        predicted = torch.round(out.squeeze().clone().detach())\n",
    "        #predicted = out.clone().detach()\n",
    "        #predicted = torch.tensor(out, device=device)\n",
    "        #print('-------- OUT')\n",
    "        #print(out)        \n",
    "        #print('--------- OUT squeeze')\n",
    "        #print(out.squeeze(),)       \n",
    "        #print('PREDICTED')\n",
    "        #print(predicted)\n",
    "        #print('TARGET')\n",
    "        #print(target)\n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_acc += acc.item()\n",
    "        mae = torch.mean(torch.abs(predicted - target).float())\n",
    "        train_mae += mae.item()\n",
    "        \n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        del feature, target, predicted\n",
    "\n",
    "    history['train_loss'].append(train_loss / len(train_iterator))\n",
    "    history['train_acc'].append(train_acc / len(train_iterator))\n",
    "    history['train_mae'].append(train_mae / len(train_iterator))\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_mae = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for id_, (feature, target) in enumerate(valid_iterator):\n",
    "            epochloop.set_postfix_str(f'Validation batch {id_}/{len(valid_iterator)}')\n",
    "            \n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            out = model(feature)\n",
    "\n",
    "            #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "            \n",
    "            predicted = torch.round(out.clone().detach())\n",
    "            #predicted = out.clone().detach()\n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            val_acc += acc.item()\n",
    "            mae = torch.mean(torch.abs(predicted - target).float())\n",
    "            val_mae += mae.item()\n",
    "            \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            del feature, target, predicted\n",
    "\n",
    "        history['val_loss'].append(val_loss / len(valid_iterator))\n",
    "        history['val_acc'].append(val_acc / len(valid_iterator))\n",
    "        history['val_mae'].append(val_mae / len(valid_iterator))\n",
    "    \n",
    "    # Возвращаем модель в режим обучения\n",
    "    # Возвращаем модель в режим обучения\n",
    "    model.train()\n",
    "\n",
    "    info_str = f'Val Loss: {val_loss / len(valloader):.3f} '\n",
    "    info_str += f'| Val mae: {val_mae / len(valloader):.3f}'\n",
    "    epochloop.set_postfix_str(info_str)\n",
    "\n",
    "    if (e+1) % print_every == 0:\n",
    "        info_str = f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} '\n",
    "        info_str += f'Train mae: {train_mae / len(trainloader):.3f} '\n",
    "        info_str += f'| Val Loss: {val_loss / len(valloader):.3f} '\n",
    "        info_str += f'Val mae: {val_mae / len(valloader):.3f}'\n",
    "        \n",
    "        epochloop.write(info_str)\n",
    "        epochloop.update()\n",
    "\n",
    "    if val_loss / len(valloader) <= val_loss_min:\n",
    "        torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "        val_loss_min = val_loss / len(valloader)\n",
    "        es_trigger = 0\n",
    "    else:\n",
    "        info_str = '[WARNING] Validation loss did not improved ('\n",
    "        info_str += f'{val_loss_min:.3f} --> {val_loss / len(valloader):.3f})'\n",
    "        \n",
    "        epochloop.write(info_str)\n",
    "        es_trigger += 1\n",
    "\n",
    "    if es_trigger >= es_limit:\n",
    "        epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "        history['epochs'] = e+1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = train_iterator\n",
    "valloader = valid_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_mogel_explore(learn_rate=0.0001, epoch=8, layers=2, drop=0.25):\n",
    "\n",
    "    vocab_size = len(word2int)\n",
    "    output_size = 1\n",
    "    embedding_size = 256\n",
    "    hidden_size = 512\n",
    "    n_layers = layers\n",
    "    dropout = drop\n",
    "    \n",
    "    model = SentimentModel(vocab_size, \n",
    "                           output_size, \n",
    "                           hidden_size, \n",
    "                           embedding_size, \n",
    "                           n_layers, \n",
    "                           dropout)\n",
    "    print(model)\n",
    "    \n",
    "    lr = learn_rate\n",
    "    criterion = nn.MSELoss()\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    grad_clip = 5\n",
    "    epochs = epoch\n",
    "    print_every = 1\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mae': [],\n",
    "        'val_loss': [],\n",
    "        'val_mae': [],\n",
    "        'epochs': epochs\n",
    "    }\n",
    "    es_limit = 5\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "    \n",
    "    # early stop trigger\n",
    "    es_trigger = 0\n",
    "    #val_loss_min = torch.inf\n",
    "    val_loss_min = 1000\n",
    "    \n",
    "    for e in epochloop:\n",
    "    \n",
    "        # Обучение\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "        train_loss = 0\n",
    "        train_mae = 0\n",
    "    \n",
    "        for id_, (feature, target) in enumerate(trainloader):\n",
    "            epochloop.set_postfix_str(f'Training batch {id_}/{len(trainloader)}')\n",
    "    \n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "    \n",
    "            optim.zero_grad()\n",
    "    \n",
    "            out = model(feature)\n",
    "            #print(out[:5])\n",
    "            #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "            predicted = torch.tensor(out, device=device)\n",
    "            #print(predicted[:5])\n",
    "            #print(target[:5])\n",
    "            #equals = predicted == target\n",
    "            #acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            mae = torch.mean(torch.abs(target - predicted))\n",
    "            train_mae += mae.item()\n",
    "    \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "    \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "            optim.step()\n",
    "    \n",
    "            del feature, target, predicted\n",
    "    \n",
    "        history['train_loss'].append(train_loss / len(trainloader))\n",
    "        history['train_mae'].append(train_mae / len(trainloader))\n",
    "    \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for id_, (feature, target) in enumerate(valloader):\n",
    "                epochloop.set_postfix_str(f'Validation batch {id_}/{len(valloader)}')\n",
    "                \n",
    "                feature, target = feature.to(device), target.to(device)\n",
    "    \n",
    "                out = model(feature)\n",
    "                #print(out[:5])\n",
    "    \n",
    "                #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "                predicted = torch.tensor(out, device=device)\n",
    "                #equals = predicted == target\n",
    "                #acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "                mae = torch.mean(torch.abs(target - predicted))\n",
    "                val_mae += mae.item()\n",
    "    \n",
    "                loss = criterion(out.squeeze(), target.float())\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                del feature, target, predicted\n",
    "    \n",
    "            history['val_loss'].append(val_loss / len(valloader))\n",
    "            history['val_mae'].append(val_mae / len(valloader))\n",
    "        \n",
    "        # Возвращаем модель в режим обучения\n",
    "        model.train()\n",
    "    \n",
    "        info_str = f'Val Loss: {val_loss / len(valloader):.3f} '\n",
    "        info_str += f'| Val mae: {val_mae / len(valloader):.3f}'\n",
    "        epochloop.set_postfix_str(info_str)\n",
    "    \n",
    "        if (e+1) % print_every == 0:\n",
    "            info_str = f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} '\n",
    "            info_str += f'Train mae: {train_mae / len(trainloader):.3f} '\n",
    "            info_str += f'| Val Loss: {val_loss / len(valloader):.3f} '\n",
    "            info_str += f'Val mae: {val_mae / len(valloader):.3f}'\n",
    "            \n",
    "            epochloop.write(info_str)\n",
    "            epochloop.update()\n",
    "\n",
    "        if val_loss / len(valloader) <= val_loss_min:\n",
    "            torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "            val_loss_min = val_loss / len(valloader)\n",
    "            es_trigger = 0\n",
    "        else:\n",
    "            info_str = '[WARNING] Validation loss did not improved ('\n",
    "            info_str += f'{val_loss_min:.3f} --> {val_loss / len(valloader):.3f})'\n",
    "            \n",
    "            epochloop.write(info_str)\n",
    "            es_trigger += 1\n",
    "    \n",
    "        if es_trigger >= es_limit:\n",
    "            epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "            history['epochs'] = e+1\n",
    "            break\n",
    "            \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(range(history['epochs']), history['train_mae'], label='Train mae')\n",
    "    plt.plot(range(history['epochs']), history['val_mae'], label='Val mae')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(range(history['epochs']), history['train_loss'], label='Train Loss')\n",
    "    plt.plot(range(history['epochs']), history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # metrics\n",
    "    test_loss = 0\n",
    "    test_mae = 0\n",
    "    \n",
    "    all_target = []\n",
    "    all_predicted = []\n",
    "    \n",
    "    testloop = tqdm(testloader, leave=True, desc='Inference')\n",
    "    with torch.no_grad():\n",
    "        for feature, target in testloop:\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "    \n",
    "            out = model(feature)\n",
    "            \n",
    "            predicted = torch.tensor(out, device=device)\n",
    "            mae = torch.mean(torch.abs(target - predicted))\n",
    "            test_mae += mae.item()\n",
    "    \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "            all_target.extend(target.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    print(f'mae: {test_mae/len(testloader):.4f}, Loss: {test_loss/len(testloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentModel(\n",
      "  (embedding): Embedding(29993, 256)\n",
      "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/8 [00:00<?, ?it/s, Training batch 0/110]/tmp/ipykernel_2361791/2189445728.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predicted = torch.tensor(out, device=device)\n",
      "Training:   0%|          | 0/8 [00:05<?, ?it/s, Validation batch 0/24] /tmp/ipykernel_2361791/2189445728.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predicted = torch.tensor(out, device=device)\n",
      "Training:  25%|██▌       | 2/8 [00:06<00:36,  6.04s/it, Training batch 3/110]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 | Train Loss: 3.015 Train mae: 1.335 | Val Loss: 1.549 Val mae: 0.992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 4/8 [00:12<00:15,  3.84s/it, Training batch 4/110]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8 | Train Loss: 1.676 Train mae: 1.053 | Val Loss: 1.558 Val mae: 1.006\n",
      "[WARNING] Validation loss did not improved (1.549 --> 1.558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 2/8 [00:14<00:44,  7.39s/it, Training batch 50/110]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrnn_mogel_explore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 73\u001b[0m, in \u001b[0;36mrnn_mogel_explore\u001b[0;34m(learn_rate, epoch, layers, drop)\u001b[0m\n\u001b[1;32m     70\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     71\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 73\u001b[0m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m feature, target, predicted\n",
      "File \u001b[0;32m~/projects/ds/nlp-basics/.venv/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:19\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/projects/ds/nlp-basics/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:149\u001b[0m, in \u001b[0;36m_NoParamDecoratorContextManager.__new__\u001b[0;34m(cls, orig_func)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_NoParamDecoratorContextManager\u001b[39;00m(_DecoratorContextManager):\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Allow a context manager to be used as a decorator without parentheses.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, orig_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m orig_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_mogel_explore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
