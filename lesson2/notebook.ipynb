{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2\n",
    "\n",
    "### Описание\n",
    "\n",
    "В вашем распоряжении датасет с русскоязычными отзывами о мобильных телефонах с выставленным рейтингом от 1 до 5.\n",
    "Ключевая задача – обучить любую модель регрессии (или классификации, если решите таким путём пойти) из пакетов scikit, XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "\n",
    "Необходимая метрика:\n",
    "\n",
    "1. Со звёздочкой (дополнительный балл) – MAE <= 0.5\n",
    "2. Минимальное допустимое значение – МАЕ <= 1.0\n",
    "\n",
    "### Что необходимо сделать\n",
    "\n",
    "1. Откройте датасет\n",
    "2. Разделите на обучение и тест\n",
    "3. Осуществите лемматизацию с помощью любого из озвученных на занятии инструментов \n",
    "4. Обучение одну или несколько моделей машинного обучения на разных представлениях данных\n",
    "5. Валидируйте модель. Если модель соответствует условиям метрик, то работа завершена. В ином случае, экспериментируйте, начиная с пункта 7. \n",
    "6. По всем попыткам обучить качественную модель пишите свои выводы и замечания, почему так получилось.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0. Импорт библиотк, определение констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3d touch просто восхитительная вещь  заряд дер...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3d touch просто восхитительный вещь заряд держ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>отключается при температуре близкой к нулю  не...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>отключаться температура близкий нуль непонятно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>в apple окончательно решили не заморачиваться ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>apple окончательно решить не заморачиваться де...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>постарался наиболее ёмко и коротко описать все...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>постараться наиболее ёмко коротко описать всё ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>достойный телефон  пользоваться одно удовольст...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>достойный телефон пользоваться удовольствие</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  \\\n",
       "0  3d touch просто восхитительная вещь  заряд дер...     5.0   \n",
       "1  отключается при температуре близкой к нулю  не...     4.0   \n",
       "2  в apple окончательно решили не заморачиваться ...     3.0   \n",
       "3  постарался наиболее ёмко и коротко описать все...     4.0   \n",
       "4  достойный телефон  пользоваться одно удовольст...     5.0   \n",
       "\n",
       "                                               lemma  \n",
       "0  3d touch просто восхитительный вещь заряд держ...  \n",
       "1  отключаться температура близкий нуль непонятно...  \n",
       "2  apple окончательно решить не заморачиваться де...  \n",
       "3  постараться наиболее ёмко коротко описать всё ...  \n",
       "4        достойный телефон пользоваться удовольствие  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"data/data_lemma_cleared.csv\"):\n",
    "    df = pd.read_csv(\"data/data_lemma_cleared.csv\", engine='python')\n",
    "\n",
    "df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319791, 2)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Review', axis=1)\n",
    "df = df.dropna()\n",
    "df.columns = ['label', 'review']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оставлю только небольшой кусок данных на время разработки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d',\n",
       " 'touch',\n",
       " 'просто',\n",
       " 'восхитительный',\n",
       " 'вещь',\n",
       " 'заряд',\n",
       " 'держать',\n",
       " 'целый',\n",
       " 'день',\n",
       " 'розовый']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = df.review.values\n",
    "words = ' '.join(reviews)\n",
    "words = words.split()\n",
    "\n",
    "print(len(words))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "int2word[0] = '<PAD>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29993"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 90949.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1422, 1208, 22, 4203, 422]\n",
      "[956, 1888, 786, 2370, 1226]\n",
      "[385, 1889, 141, 1, 2433]\n",
      "[1820, 2306, 13335, 4206, 876]\n",
      "[217, 2, 21, 543]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_enc = []\n",
    "\n",
    "for review in tqdm(reviews):\n",
    "    reviews_enc += [[]]\n",
    "    \n",
    "    for word in review.split():\n",
    "        reviews_enc[-1].append(word2int[word])\n",
    "\n",
    "for i in range(5):\n",
    "    print(reviews_enc[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 256)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    features = np.full((len(reviews), \n",
    "                        seq_length), \n",
    "                       pad_id, \n",
    "                       dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "seq_length = 256\n",
    "features = pad_features(reviews_enc, \n",
    "                        pad_id=word2int['<PAD>'], \n",
    "                        seq_length=seq_length)\n",
    "\n",
    "assert len(features) == len(reviews_enc)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 4., 3., ..., 5., 5., 4.])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.label.to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "===============\n",
      "Train set: (7000, 256)\n",
      "Validation set: (1500, 256)\n",
      "Test set: (1500, 256)\n"
     ]
    }
   ],
   "source": [
    "train_size = .7\n",
    "val_size = .5\n",
    "\n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, remain_x = features[:split_id], features[split_id:]\n",
    "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "split_val_id = int(len(remain_x) * val_size)\n",
    "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
    "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n",
    "\n",
    "print('Feature Shapes:')\n",
    "print('===============')\n",
    "print('Train set: {}'.format(train_x.shape))\n",
    "print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 64\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), \n",
    "                         torch.from_numpy(train_y))\n",
    "\n",
    "\n",
    "validset = TensorDataset(torch.from_numpy(val_x), \n",
    "                         torch.from_numpy(val_y))\n",
    "\n",
    "testset = TensorDataset(torch.from_numpy(test_x), \n",
    "                        torch.from_numpy(test_y))\n",
    "\n",
    "train_iterator = DataLoader(trainset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "valid_iterator = DataLoader(validset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "test_iterator = DataLoader(testset, \n",
    "                           shuffle=True, \n",
    "                           batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция подсчета accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    error = torch.mean(torch.abs(rounded_preds - y).float())\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch[0].T.cuda()).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions.float(), \n",
    "                          batch[1].float().cuda())\n",
    "        \n",
    "        acc = mae(predictions.float(), \n",
    "                              batch[1].float().cuda())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0].T.cuda()).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions.float(), \n",
    "                              batch[1].float().cuda())\n",
    "            \n",
    "            acc = mae(predictions.float(), \n",
    "                                  batch[1].float().cuda())\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_dim, \n",
    "                 n_filters, \n",
    "                 filter_sizes, \n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embedding_dim)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[0], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[1], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[2], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, \n",
    "                            output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = [sent len, batch size]\n",
    "        x = x.permute(1, 0)\n",
    "\n",
    "        #x = [batch size, sent len]\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = len(word2int)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            N_FILTERS, \n",
    "            FILTER_SIZES, \n",
    "            OUTPUT_DIM, \n",
    "            DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(29993, 100)\n",
       "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(model, \n",
    "                                       train_iterator, \n",
    "                                       optimizer, \n",
    "                                       criterion)\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate_func(model, \n",
    "                                          valid_iterator, \n",
    "                                          criterion)\n",
    "    \n",
    "    train_msg = f'Epoch: {epoch+1:02}, '\n",
    "    train_msg += f'Train Loss: {train_loss:.3f}, '\n",
    "    train_msg += f'Train Acc: {train_acc:.2f}, '\n",
    "    train_msg += f'Val. Loss: {valid_loss:.3f}, '\n",
    "    train_msg += f'Val. Acc: {valid_acc:.2f}'\n",
    "    \n",
    "    print(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 27.158, Test Acc: 5.03\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss , test_acc = evaluate_func(model, \n",
    "                                     test_iterator, \n",
    "                                     criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch size:  torch.Size([64, 256])\n",
      "Sample batch input: \n",
      " tensor([[    2,    22,   783,  ...,     0,     0,     0],\n",
      "        [  328,   876,   113,  ...,     0,     0,     0],\n",
      "        [    3,    90,  2723,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [   66,   183,   672,  ...,     0,     0,     0],\n",
      "        [  406,   111,   277,  ...,     0,     0,     0],\n",
      "        [    6,     2, 11620,  ...,     0,     0,     0]])\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label input: \n",
      " tensor([5., 3., 5., 5., 2., 5., 5., 4., 5., 1., 4., 5., 4., 4., 4., 1., 5., 5.,\n",
      "        3., 2., 4., 5., 5., 5., 3., 4., 5., 5., 4., 5., 2., 5., 5., 5., 5., 5.,\n",
      "        5., 4., 3., 5., 3., 2., 5., 1., 3., 4., 5., 5., 4., 4., 3., 4., 5., 5.,\n",
      "        5., 4., 5., 4., 5., 4., 3., 3., 5., 4.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_iterator)\n",
    "x, y = next(dataiter)\n",
    "\n",
    "print('Sample batch size: ', x.size()) \n",
    "print('Sample batch input: \\n', x)\n",
    "print()\n",
    "print('Sample label size: ', y.size())\n",
    "print('Sample label input: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_weights = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "        nn.init.uniform_(self.attn_weights, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Тензор размерности (batch_size, max_length, hidden_size).\n",
    "        Returns:\n",
    "            torch.Tensor: Взвешенная сумма encoder_outputs с учетом весов Attention.\n",
    "        \"\"\"\n",
    "        # Рассчитываем веса Attention\n",
    "        attn_energies = torch.bmm(encoder_outputs, self.attn_weights.unsqueeze(0).expand(encoder_outputs.size(0), *self.attn_weights.size()))\n",
    "        attn_weights = F.softmax(attn_energies, dim=1)\n",
    "\n",
    "        # Выполняем взвешенную сумму\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), encoder_outputs)\n",
    "\n",
    "        return context.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size=128, \n",
    "                 embedding_size=400, n_layers=2, dropout=0.2):\n",
    "        \n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # convert feature to long\n",
    "        x = x.long()\n",
    "\n",
    "        # map input to vector\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # pass forward to lstm\n",
    "        x, _ =  self.lstm(x)\n",
    "        #x = x[:, -1, :]\n",
    "        \n",
    "        x = self.attention(x)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)     \n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [00:08<00:25,  8.48s/it, Training batch 3/110]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | TRAIN Loss: 9.258  mae: 2.245  acc: 0.156 | VAL Loss: 1.911  mae: 1.257 acc: 0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 4/5 [00:17<00:05,  5.39s/it, Training batch 3/110]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | TRAIN Loss: 2.306  mae: 1.198  acc: 0.244 | VAL Loss: 7.074  mae: 2.438 acc: 0.045\n",
      "[WARNING] Validation loss did not improved (1.911 --> 7.074)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:26<00:00,  4.90s/it, Training batch 3/110]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | TRAIN Loss: 2.095  mae: 1.139  acc: 0.258 | VAL Loss: 3.404  mae: 1.512 acc: 0.271\n",
      "[WARNING] Validation loss did not improved (1.911 --> 3.404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 7it [00:34,  5.96s/it, Training batch 3/110]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | TRAIN Loss: 1.826  mae: 1.072  acc: 0.267 | VAL Loss: 1.792  mae: 1.223 acc: 0.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5/5 [00:43<00:00,  8.75s/it, Val Loss: 1.685 | Val mae: 1.061]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | TRAIN Loss: 1.756  mae: 1.039  acc: 0.283 | VAL Loss: 1.685  mae: 1.061 acc: 0.296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'train_mae': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_mae': [],\n",
    "    'epochs': epochs\n",
    "}\n",
    "\n",
    "es_limit = 5\n",
    "vocab_size = len(word2int)\n",
    "output_size = 1\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 3\n",
    "dropout=0.25\n",
    "epochs = 5\n",
    "grad_clip = 1\n",
    "print_every = 1\n",
    "\n",
    "model = SentimentModel(vocab_size, \n",
    "                       output_size, \n",
    "                       hidden_size, \n",
    "                       embedding_size, \n",
    "                       n_layers, \n",
    "                       dropout)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "\n",
    "# early stop trigger\n",
    "es_trigger = 0\n",
    "val_loss_min = 1000 #torch.inf\n",
    "\n",
    "for e in epochloop:\n",
    "\n",
    "    # Обучение\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_mae = 0\n",
    "    \n",
    "    for id_, (feature, target) in enumerate(train_iterator):\n",
    "        \n",
    "        epochloop.set_postfix_str(f'Training batch {id_}/{len(train_iterator)}')\n",
    "\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        out = model(feature)\n",
    "        #print(out[:5])\n",
    "        #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        #predicted = torch.tensor(torch.round(out), device=device)\n",
    "        predicted = torch.round(out.squeeze().clone().detach())\n",
    "        #predicted = out.clone().detach()\n",
    "        #predicted = torch.tensor(out, device=device)\n",
    "        #print('-------- OUT')\n",
    "        #print(out)        \n",
    "        #print('--------- OUT squeeze')\n",
    "        #print(out.squeeze(),)       \n",
    "        #print('PREDICTED')\n",
    "        #print(predicted)\n",
    "        #print('TARGET')\n",
    "        #print(target)\n",
    "        \n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "        mae = torch.mean(torch.abs(predicted - target).float())\n",
    "        train_mae += mae.item()\n",
    "        \n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        del feature, target, predicted\n",
    "\n",
    "    history['train_loss'].append(train_loss / len(train_iterator))\n",
    "    history['train_acc'].append(train_acc / len(train_iterator))\n",
    "    history['train_mae'].append(train_mae / len(train_iterator))\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_mae = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for id_, (feature, target) in enumerate(valid_iterator):\n",
    "            epochloop.set_postfix_str(f'Validation batch {id_}/{len(valid_iterator)}')\n",
    "            \n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            out = model(feature)\n",
    "\n",
    "             \n",
    "            predicted = torch.round(out.clone().detach())\n",
    "            \n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            val_acc += acc.item()\n",
    "            \n",
    "            mae = torch.mean(torch.abs(predicted - target).float())\n",
    "            val_mae += mae.item()\n",
    "            \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            del feature, target, predicted\n",
    "\n",
    "        history['val_loss'].append(val_loss / len(valid_iterator))\n",
    "        history['val_acc'].append(val_acc / len(valid_iterator))\n",
    "        history['val_mae'].append(val_mae / len(valid_iterator))\n",
    "    \n",
    "    # Возвращаем модель в режим обучения\n",
    "    # Возвращаем модель в режим обучения\n",
    "    model.train()\n",
    "\n",
    "    info_str = f'Val Loss: {val_loss / len(valid_iterator):.3f} '\n",
    "    info_str += f'| Val mae: {val_mae / len(valid_iterator):.3f}'\n",
    "    epochloop.set_postfix_str(info_str)\n",
    "\n",
    "    if (e+1) % print_every == 0:\n",
    "        info_str = f'Epoch {e+1}/{epochs} | TRAIN Loss: {train_loss / len(train_iterator):.3f} '\n",
    "        info_str += f' mae: {train_mae / len(train_iterator):.3f} '\n",
    "        info_str += f' acc: {train_acc / len(train_iterator):.3f} '\n",
    "        info_str += f'| VAL Loss: {val_loss / len(valid_iterator):.3f} '\n",
    "        info_str += f' mae: {val_mae / len(valid_iterator):.3f}'\n",
    "        info_str += f' acc: {val_acc / len(valid_iterator):.3f}'\n",
    "        \n",
    "        epochloop.write(info_str)\n",
    "        epochloop.update()\n",
    "\n",
    "    if val_loss / len(valid_iterator) <= val_loss_min:\n",
    "        #torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "        val_loss_min = val_loss / len(valid_iterator)\n",
    "        es_trigger = 0\n",
    "    else:\n",
    "        info_str = '[WARNING] Validation loss did not improved ('\n",
    "        info_str += f'{val_loss_min:.3f} --> {val_loss / len(valid_iterator):.3f})'\n",
    "        \n",
    "        epochloop.write(info_str)\n",
    "        es_trigger += 1\n",
    "\n",
    "    if es_trigger >= es_limit:\n",
    "        epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "        history['epochs'] = e+1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = train_iterator\n",
    "valloader = valid_iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_mogel_explore(learn_rate=0.0001, epoch=8, layers=2, drop=0.25):\n",
    "\n",
    "    vocab_size = len(word2int)\n",
    "    output_size = 1\n",
    "    embedding_size = 256\n",
    "    hidden_size = 512\n",
    "    n_layers = layers\n",
    "    dropout = drop\n",
    "    \n",
    "    model = SentimentModel(vocab_size, \n",
    "                           output_size, \n",
    "                           hidden_size, \n",
    "                           embedding_size, \n",
    "                           n_layers, \n",
    "                           dropout)\n",
    "    print(model)\n",
    "    \n",
    "    lr = learn_rate\n",
    "    criterion = nn.MSELoss()\n",
    "    optim = Adam(model.parameters(), lr=lr)\n",
    "    grad_clip = 5\n",
    "    epochs = epoch\n",
    "    print_every = 1\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_mae': [],\n",
    "        'val_loss': [],\n",
    "        'val_mae': [],\n",
    "        'epochs': epochs\n",
    "    }\n",
    "    es_limit = 5\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "    \n",
    "    # early stop trigger\n",
    "    es_trigger = 0\n",
    "    #val_loss_min = torch.inf\n",
    "    val_loss_min = 1000\n",
    "    \n",
    "    for e in epochloop:\n",
    "    \n",
    "        # Обучение\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "        train_loss = 0\n",
    "        train_mae = 0\n",
    "    \n",
    "        for id_, (feature, target) in enumerate(trainloader):\n",
    "            epochloop.set_postfix_str(f'Training batch {id_}/{len(trainloader)}')\n",
    "    \n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "    \n",
    "            optim.zero_grad()\n",
    "    \n",
    "            out = model(feature)\n",
    "            #print(out[:5])\n",
    "            #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "            predicted = torch.tensor(out, device=device)\n",
    "            #print(predicted[:5])\n",
    "            #print(target[:5])\n",
    "            #equals = predicted == target\n",
    "            #acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            mae = torch.mean(torch.abs(target - predicted))\n",
    "            train_mae += mae.item()\n",
    "    \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "    \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "            optim.step()\n",
    "    \n",
    "            del feature, target, predicted\n",
    "    \n",
    "        history['train_loss'].append(train_loss / len(trainloader))\n",
    "        history['train_mae'].append(train_mae / len(trainloader))\n",
    "    \n",
    "        # Валидация\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0\n",
    "        val_mae = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for id_, (feature, target) in enumerate(valloader):\n",
    "                epochloop.set_postfix_str(f'Validation batch {id_}/{len(valloader)}')\n",
    "                \n",
    "                feature, target = feature.to(device), target.to(device)\n",
    "    \n",
    "                out = model(feature)\n",
    "                #print(out[:5])\n",
    "    \n",
    "                #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "                predicted = torch.tensor(out, device=device)\n",
    "                #equals = predicted == target\n",
    "                #acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "                mae = torch.mean(torch.abs(target - predicted))\n",
    "                val_mae += mae.item()\n",
    "    \n",
    "                loss = criterion(out.squeeze(), target.float())\n",
    "                val_loss += loss.item()\n",
    "    \n",
    "                del feature, target, predicted\n",
    "    \n",
    "            history['val_loss'].append(val_loss / len(valloader))\n",
    "            history['val_mae'].append(val_mae / len(valloader))\n",
    "        \n",
    "        # Возвращаем модель в режим обучения\n",
    "        model.train()\n",
    "    \n",
    "        info_str = f'Val Loss: {val_loss / len(valloader):.3f} '\n",
    "        info_str += f'| Val mae: {val_mae / len(valloader):.3f}'\n",
    "        epochloop.set_postfix_str(info_str)\n",
    "    \n",
    "        if (e+1) % print_every == 0:\n",
    "            info_str = f'Epoch {e+1}/{epochs} | Train Loss: {train_loss / len(trainloader):.3f} '\n",
    "            info_str += f'Train mae: {train_mae / len(trainloader):.3f} '\n",
    "            info_str += f'| Val Loss: {val_loss / len(valloader):.3f} '\n",
    "            info_str += f'Val mae: {val_mae / len(valloader):.3f}'\n",
    "            \n",
    "            epochloop.write(info_str)\n",
    "            epochloop.update()\n",
    "\n",
    "        if val_loss / len(valloader) <= val_loss_min:\n",
    "            torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "            val_loss_min = val_loss / len(valloader)\n",
    "            es_trigger = 0\n",
    "        else:\n",
    "            info_str = '[WARNING] Validation loss did not improved ('\n",
    "            info_str += f'{val_loss_min:.3f} --> {val_loss / len(valloader):.3f})'\n",
    "            \n",
    "            epochloop.write(info_str)\n",
    "            es_trigger += 1\n",
    "    \n",
    "        if es_trigger >= es_limit:\n",
    "            epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "            history['epochs'] = e+1\n",
    "            break\n",
    "            \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(range(history['epochs']), history['train_mae'], label='Train mae')\n",
    "    plt.plot(range(history['epochs']), history['val_mae'], label='Val mae')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(range(history['epochs']), history['train_loss'], label='Train Loss')\n",
    "    plt.plot(range(history['epochs']), history['val_loss'], label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # metrics\n",
    "    test_loss = 0\n",
    "    test_mae = 0\n",
    "    \n",
    "    all_target = []\n",
    "    all_predicted = []\n",
    "    \n",
    "    testloop = tqdm(testloader, leave=True, desc='Inference')\n",
    "    with torch.no_grad():\n",
    "        for feature, target in testloop:\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "    \n",
    "            out = model(feature)\n",
    "            \n",
    "            predicted = torch.tensor(out, device=device)\n",
    "            mae = torch.mean(torch.abs(target - predicted))\n",
    "            test_mae += mae.item()\n",
    "    \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "            all_target.extend(target.cpu().numpy())\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    print(f'mae: {test_mae/len(testloader):.4f}, Loss: {test_loss/len(testloader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_mogel_explore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
