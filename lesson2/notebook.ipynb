{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2\n",
    "\n",
    "### Описание\n",
    "\n",
    "В вашем распоряжении датасет с русскоязычными отзывами о мобильных телефонах с выставленным рейтингом от 1 до 5.\n",
    "Ключевая задача – обучить любую модель регрессии (или классификации, если решите таким путём пойти) из пакетов scikit, XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "\n",
    "Необходимая метрика:\n",
    "\n",
    "1. Со звёздочкой (дополнительный балл) – MAE <= 0.5\n",
    "2. Минимальное допустимое значение – МАЕ <= 1.0\n",
    "\n",
    "### Что необходимо сделать\n",
    "\n",
    "1. Откройте датасет\n",
    "2. Разделите на обучение и тест\n",
    "3. Осуществите лемматизацию с помощью любого из озвученных на занятии инструментов \n",
    "4. Обучение одну или несколько моделей машинного обучения на разных представлениях данных\n",
    "5. Валидируйте модель. Если модель соответствует условиям метрик, то работа завершена. В ином случае, экспериментируйте, начиная с пункта 7. \n",
    "6. По всем попыткам обучить качественную модель пишите свои выводы и замечания, почему так получилось.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0. Импорт библиотк, определение констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/tiv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3d touch просто восхитительная вещь  заряд дер...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3d touch просто восхитительный вещь заряд держ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>отключается при температуре близкой к нулю  не...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>отключаться температура близкий нуль непонятно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>в apple окончательно решили не заморачиваться ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>apple окончательно решить не заморачиваться де...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>постарался наиболее ёмко и коротко описать все...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>постараться наиболее ёмко коротко описать всё ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>достойный телефон  пользоваться одно удовольст...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>достойный телефон пользоваться удовольствие</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  \\\n",
       "0  3d touch просто восхитительная вещь  заряд дер...     5.0   \n",
       "1  отключается при температуре близкой к нулю  не...     4.0   \n",
       "2  в apple окончательно решили не заморачиваться ...     3.0   \n",
       "3  постарался наиболее ёмко и коротко описать все...     4.0   \n",
       "4  достойный телефон  пользоваться одно удовольст...     5.0   \n",
       "\n",
       "                                               lemma  \n",
       "0  3d touch просто восхитительный вещь заряд держ...  \n",
       "1  отключаться температура близкий нуль непонятно...  \n",
       "2  apple окончательно решить не заморачиваться де...  \n",
       "3  постараться наиболее ёмко коротко описать всё ...  \n",
       "4        достойный телефон пользоваться удовольствие  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"data/data_lemma_cleared.csv\"):\n",
    "    df = pd.read_csv(\"data/data_lemma_cleared.csv\", engine='python')\n",
    "\n",
    "df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319791, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Review', axis=1)\n",
    "df = df.dropna()\n",
    "df.columns = ['label', 'review']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оставлю только небольшой кусок данных на время разработки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4356550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3d',\n",
       " 'touch',\n",
       " 'просто',\n",
       " 'восхитительный',\n",
       " 'вещь',\n",
       " 'заряд',\n",
       " 'держать',\n",
       " 'целый',\n",
       " 'день',\n",
       " 'розовый']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = df.review.values\n",
    "words = ' '.join(reviews)\n",
    "words = words.split()\n",
    "\n",
    "print(len(words))\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(words)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "int2word = dict(enumerate(vocab, 1))\n",
    "int2word[0] = '<PAD>'\n",
    "word2int = {word: id for id, word in int2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118745"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 104252.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1382, 1594, 22, 5659, 421]\n",
      "[1050, 2137, 803, 2558, 1325]\n",
      "[434, 1979, 133, 1, 2228]\n",
      "[1686, 2872, 47509, 4322, 798]\n",
      "[204, 2, 20, 629]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_enc = []\n",
    "\n",
    "for review in tqdm(reviews):\n",
    "    reviews_enc += [[]]\n",
    "    \n",
    "    for word in review.split():\n",
    "        reviews_enc[-1].append(word2int[word])\n",
    "\n",
    "for i in range(5):\n",
    "    print(reviews_enc[i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_features(reviews, pad_id, seq_length=128):\n",
    "    features = np.full((len(reviews), \n",
    "                        seq_length), \n",
    "                       pad_id, \n",
    "                       dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews):\n",
    "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
    "\n",
    "    return features\n",
    "\n",
    "seq_length = 256\n",
    "features = pad_features(reviews_enc, \n",
    "                        pad_id=word2int['<PAD>'], \n",
    "                        seq_length=seq_length)\n",
    "\n",
    "assert len(features) == len(reviews_enc)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 4., 3., ..., 4., 1., 5.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df.label.to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Shapes:\n",
      "===============\n",
      "Train set: (70000, 256)\n",
      "Validation set: (15000, 256)\n",
      "Test set: (15000, 256)\n"
     ]
    }
   ],
   "source": [
    "train_size = .7\n",
    "val_size = .5\n",
    "\n",
    "split_id = int(len(features) * train_size)\n",
    "train_x, remain_x = features[:split_id], features[split_id:]\n",
    "train_y, remain_y = labels[:split_id], labels[split_id:]\n",
    "\n",
    "split_val_id = int(len(remain_x) * val_size)\n",
    "val_x, test_x = remain_x[:split_val_id], remain_x[split_val_id:]\n",
    "val_y, test_y = remain_y[:split_val_id], remain_y[split_val_id:]\n",
    "\n",
    "print('Feature Shapes:')\n",
    "print('===============')\n",
    "print('Train set: {}'.format(train_x.shape))\n",
    "print('Validation set: {}'.format(val_x.shape))\n",
    "print('Test set: {}'.format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 64\n",
    "\n",
    "trainset = TensorDataset(torch.from_numpy(train_x), \n",
    "                         torch.from_numpy(train_y))\n",
    "\n",
    "\n",
    "validset = TensorDataset(torch.from_numpy(val_x), \n",
    "                         torch.from_numpy(val_y))\n",
    "\n",
    "testset = TensorDataset(torch.from_numpy(test_x), \n",
    "                        torch.from_numpy(test_y))\n",
    "\n",
    "train_iterator = DataLoader(trainset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "valid_iterator = DataLoader(validset, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size)\n",
    "\n",
    "test_iterator = DataLoader(testset, \n",
    "                           shuffle=True, \n",
    "                           batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция подсчета accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(F.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    error = torch.mean(torch.abs(rounded_preds - y).float())\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_func(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch[0].T.cuda()).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions.float(), \n",
    "                          batch[1].float().cuda())\n",
    "        \n",
    "        acc = mae(predictions.float(), \n",
    "                              batch[1].float().cuda())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_func(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch[0].T.cuda()).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions.float(), \n",
    "                              batch[1].float().cuda())\n",
    "            \n",
    "            acc = mae(predictions.float(), \n",
    "                                  batch[1].float().cuda())\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size,\n",
    "                 embedding_dim, \n",
    "                 n_filters, \n",
    "                 filter_sizes, \n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, \n",
    "                                      embedding_dim)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[0], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[1], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, \n",
    "                                out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[2], \n",
    "                                             embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, \n",
    "                            output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = [sent len, batch size]\n",
    "        x = x.permute(1, 0)\n",
    "\n",
    "        #x = [batch size, sent len]\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "\n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = len(word2int)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            N_FILTERS, \n",
    "            FILTER_SIZES, \n",
    "            OUTPUT_DIM, \n",
    "            DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(118745, 100)\n",
       "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_func(model, \n",
    "                                       train_iterator, \n",
    "                                       optimizer, \n",
    "                                       criterion)\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate_func(model, \n",
    "                                          valid_iterator, \n",
    "                                          criterion)\n",
    "    \n",
    "    train_msg = f'Epoch: {epoch+1:02}, '\n",
    "    train_msg += f'Train Loss: {train_loss:.3f}, '\n",
    "    train_msg += f'Train Acc: {train_acc:.2f}, '\n",
    "    train_msg += f'Val. Loss: {valid_loss:.3f}, '\n",
    "    train_msg += f'Val. Acc: {valid_acc:.2f}'\n",
    "    \n",
    "    print(train_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 26.029, Test Acc: 4.89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_loss , test_acc = evaluate_func(model, \n",
    "                                     test_iterator, \n",
    "                                     criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch size:  torch.Size([64, 256])\n",
      "Sample batch input: \n",
      " tensor([[   5,   92,    2,  ...,    0,    0,    0],\n",
      "        [1032,  192,    4,  ...,    0,    0,    0],\n",
      "        [2871,   86,  121,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 257,  254,    9,  ...,    0,    0,    0],\n",
      "        [   2,   87,  164,  ...,    0,    0,    0],\n",
      "        [  25,  146,  212,  ...,    0,    0,    0]])\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label input: \n",
      " tensor([5., 5., 5., 5., 5., 5., 2., 5., 5., 5., 5., 4., 5., 5., 5., 4., 3., 3.,\n",
      "        1., 1., 4., 5., 5., 5., 3., 5., 5., 5., 4., 5., 4., 5., 4., 5., 5., 5.,\n",
      "        1., 3., 5., 5., 5., 5., 1., 4., 5., 4., 5., 5., 4., 4., 5., 2., 5., 1.,\n",
      "        5., 2., 4., 5., 3., 5., 1., 4., 1., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_iterator)\n",
    "x, y = next(dataiter)\n",
    "\n",
    "print('Sample batch size: ', x.size()) \n",
    "print('Sample batch input: \\n', x)\n",
    "print()\n",
    "print('Sample label size: ', y.size())\n",
    "print('Sample label input: \\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_weights = nn.Parameter(torch.Tensor(hidden_size, 1))\n",
    "        nn.init.uniform_(self.attn_weights, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Тензор размерности (batch_size, max_length, hidden_size).\n",
    "        Returns:\n",
    "            torch.Tensor: Взвешенная сумма encoder_outputs с учетом весов Attention.\n",
    "        \"\"\"\n",
    "        # Рассчитываем веса Attention\n",
    "        attn_energies = torch.bmm(encoder_outputs, self.attn_weights.unsqueeze(0).expand(encoder_outputs.size(0), *self.attn_weights.size()))\n",
    "        attn_weights = F.softmax(attn_energies, dim=1)\n",
    "\n",
    "        # Выполняем взвешенную сумму\n",
    "        context = torch.bmm(attn_weights.transpose(1, 2), encoder_outputs)\n",
    "\n",
    "        return context.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, hidden_size=128, \n",
    "                 embedding_size=400, n_layers=2, dropout=0.2):\n",
    "        \n",
    "        super(SentimentModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # convert feature to long\n",
    "        x = x.long()\n",
    "\n",
    "        # map input to vector\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # pass forward to lstm\n",
    "        x, _ =  self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # x = self.attention(x) не взлетело\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)     \n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)     \n",
    " \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)    \n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)    \n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)    \n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = self.dropout(x)    \n",
    "                   \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 2/10 [01:32<12:18, 92.34s/it, Training batch 3/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | TRAIN Loss: 2.597  mae: 1.259  acc: 0.231 | VAL Loss: 2.114  mae: 1.065 acc: 0.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 4/10 [03:06<05:51, 58.66s/it, Training batch 2/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | TRAIN Loss: 1.964  mae: 1.108  acc: 0.255 | VAL Loss: 1.946  mae: 1.310 acc: 0.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 5/10 [04:40<04:23, 52.69s/it, Training batch 3/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | TRAIN Loss: 1.796  mae: 1.049  acc: 0.276 | VAL Loss: 1.878  mae: 1.087 acc: 0.377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 7/10 [06:12<03:10, 63.41s/it, Training batch 2/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | TRAIN Loss: 1.460  mae: 0.927  acc: 0.315 | VAL Loss: 1.221  mae: 1.181 acc: 0.184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 8/10 [07:45<01:52, 56.01s/it, Training batch 3/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | TRAIN Loss: 1.045  mae: 0.767  acc: 0.385 | VAL Loss: 0.951  mae: 1.231 acc: 0.264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 9/10 [09:18<01:04, 64.67s/it, Training batch 3/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | TRAIN Loss: 0.879  mae: 0.695  acc: 0.429 | VAL Loss: 1.369  mae: 1.591 acc: 0.251\n",
      "[WARNING] Validation loss did not improved (0.951 --> 1.369)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 11it [10:51, 71.82s/it, Training batch 3/1094]                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | TRAIN Loss: 0.826  mae: 0.657  acc: 0.459 | VAL Loss: 0.940  mae: 1.321 acc: 0.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 12it [12:23, 61.12s/it, Training batch 3/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | TRAIN Loss: 0.797  mae: 0.641  acc: 0.470 | VAL Loss: 1.003  mae: 1.278 acc: 0.339\n",
      "[WARNING] Validation loss did not improved (0.940 --> 1.003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 13it [13:56, 68.37s/it, Training batch 3/1094]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | TRAIN Loss: 0.818  mae: 0.647  acc: 0.469 | VAL Loss: 1.017  mae: 1.295 acc: 0.310\n",
      "[WARNING] Validation loss did not improved (0.940 --> 1.017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [15:29<00:00, 92.92s/it, Val Loss: 1.153 | Val mae: 1.412]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | TRAIN Loss: 0.912  mae: 0.679  acc: 0.454 | VAL Loss: 1.153  mae: 1.412 acc: 0.200\n",
      "[WARNING] Validation loss did not improved (0.940 --> 1.153)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'train_mae': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_mae': []\n",
    "}\n",
    "\n",
    "lr = 0.003\n",
    "es_limit = 5\n",
    "vocab_size = len(word2int)\n",
    "output_size = 1\n",
    "embedding_size = 256\n",
    "hidden_size = 512\n",
    "n_layers = 3\n",
    "dropout=0.25\n",
    "epochs = 10\n",
    "grad_clip = 1\n",
    "print_every = 1\n",
    "\n",
    "model = SentimentModel(vocab_size, \n",
    "                       output_size, \n",
    "                       hidden_size, \n",
    "                       embedding_size, \n",
    "                       n_layers, \n",
    "                       dropout)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochloop = tqdm(range(epochs), position=0, desc='Training', leave=True)\n",
    "\n",
    "# early stop trigger\n",
    "es_trigger = 0\n",
    "val_loss_min = 1000 #torch.inf\n",
    "\n",
    "for e in epochloop:\n",
    "\n",
    "    # Обучение\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_mae = 0\n",
    "    \n",
    "    for id_, (feature, target) in enumerate(train_iterator):\n",
    "        \n",
    "        epochloop.set_postfix_str(f'Training batch {id_}/{len(train_iterator)}')\n",
    "\n",
    "        feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        out = model(feature)\n",
    "        #print(out[:5])\n",
    "        #predicted = torch.tensor([1 if i == True else 0 for i in out > 0.5], device=device)\n",
    "        #predicted = torch.tensor(torch.round(out), device=device)\n",
    "        predicted = torch.round(out.squeeze().clone().detach())\n",
    "        #predicted = out.clone().detach()\n",
    "        #predicted = torch.tensor(out, device=device)\n",
    "        #print('-------- OUT')\n",
    "        #print(out)        \n",
    "        #print('--------- OUT squeeze')\n",
    "        #print(out.squeeze(),)       \n",
    "        #print('PREDICTED')\n",
    "        #print(predicted)\n",
    "        #print('TARGET')\n",
    "        #print(target)\n",
    "        \n",
    "        equals = predicted == target\n",
    "        acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "        mae = torch.mean(torch.abs(predicted - target).float())\n",
    "        train_mae += mae.item()\n",
    "        \n",
    "        loss = criterion(out.squeeze(), target.float())\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        del feature, target, predicted\n",
    "\n",
    "    history['train_loss'].append(train_loss / len(train_iterator))\n",
    "    history['train_acc'].append(train_acc / len(train_iterator))\n",
    "    history['train_mae'].append(train_mae / len(train_iterator))\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_mae = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for id_, (feature, target) in enumerate(valid_iterator):\n",
    "            epochloop.set_postfix_str(f'Validation batch {id_}/{len(valid_iterator)}')\n",
    "            \n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            out = model(feature)\n",
    "\n",
    "             \n",
    "            predicted = torch.round(out.clone().detach())\n",
    "            \n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            val_acc += acc.item()\n",
    "            \n",
    "            mae = torch.mean(torch.abs(predicted - target).float())\n",
    "            val_mae += mae.item()\n",
    "            \n",
    "            loss = criterion(out.squeeze(), target.float())\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            del feature, target, predicted\n",
    "\n",
    "        history['val_loss'].append(val_loss / len(valid_iterator))\n",
    "        history['val_acc'].append(val_acc / len(valid_iterator))\n",
    "        history['val_mae'].append(val_mae / len(valid_iterator))\n",
    "    \n",
    "    # Возвращаем модель в режим обучения\n",
    "    # Возвращаем модель в режим обучения\n",
    "    model.train()\n",
    "\n",
    "    info_str = f'Val Loss: {val_loss / len(valid_iterator):.3f} '\n",
    "    info_str += f'| Val mae: {val_mae / len(valid_iterator):.3f}'\n",
    "    epochloop.set_postfix_str(info_str)\n",
    "\n",
    "    if (e+1) % print_every == 0:\n",
    "        info_str = f'Epoch {e+1}/{epochs} | TRAIN Loss: {train_loss / len(train_iterator):.3f} '\n",
    "        info_str += f' mae: {train_mae / len(train_iterator):.3f} '\n",
    "        info_str += f' acc: {train_acc / len(train_iterator):.3f} '\n",
    "        info_str += f'| VAL Loss: {val_loss / len(valid_iterator):.3f} '\n",
    "        info_str += f' mae: {val_mae / len(valid_iterator):.3f}'\n",
    "        info_str += f' acc: {val_acc / len(valid_iterator):.3f}'\n",
    "        \n",
    "        epochloop.write(info_str)\n",
    "        epochloop.update()\n",
    "\n",
    "    if val_loss / len(valid_iterator) <= val_loss_min:\n",
    "        #torch.save(model.state_dict(), './sentiment_lstm.pt')\n",
    "        val_loss_min = val_loss / len(valid_iterator)\n",
    "        es_trigger = 0\n",
    "    else:\n",
    "        info_str = '[WARNING] Validation loss did not improved ('\n",
    "        info_str += f'{val_loss_min:.3f} --> {val_loss / len(valid_iterator):.3f})'\n",
    "        \n",
    "        epochloop.write(info_str)\n",
    "        es_trigger += 1\n",
    "\n",
    "    if es_trigger >= es_limit:\n",
    "        epochloop.write(f'Early stopped at Epoch-{e+1}')\n",
    "        history['epochs'] = e+1\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mae: 1.380\n"
     ]
    }
   ],
   "source": [
    "test_mae = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for id_, (feature, target) in enumerate(test_iterator):\n",
    "            \n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            out = model(feature)\n",
    "            \n",
    "            predicted = torch.round(out.clone().detach())\n",
    "            \n",
    "            equals = predicted == target\n",
    "            acc = torch.mean(equals.type(torch.FloatTensor))\n",
    "            val_acc += acc.item()\n",
    "            \n",
    "            mae = torch.mean(torch.abs(predicted - target).float())\n",
    "            test_mae += mae.item()\n",
    "            \n",
    "            \n",
    "print(f' mae: {test_mae / len(test_iterator):.3f}')                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
