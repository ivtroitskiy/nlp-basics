{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры скрипта\n",
    "\n",
    "# DOMAIN = 'movie'\n",
    "# DOMAIN = 'computer'\n",
    "DOMAIN = 'nature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Jaccard_metric:\n",
    "\n",
    "    def clean_list(self, A):\n",
    "        AA = []\n",
    "        if A is not None:\n",
    "            for a in A:\n",
    "                if isinstance(a, dict) and 'sub' in a.keys() and 'rel' in a.keys() and 'obj' in a.keys():\n",
    "                    AA.append(a)\n",
    "\n",
    "        return AA\n",
    "\n",
    "\n",
    "    def count_matching(self, A, B):\n",
    "        \"\"\"\n",
    "        Функция подсчитывает количество одинаковых триплетов\n",
    "        \"\"\"\n",
    "\n",
    "        count = 0\n",
    "        A = self.clean_list(A)\n",
    "        B = self.clean_list(B)\n",
    "\n",
    "        for i in range(len(A)):\n",
    "            a = A[i]\n",
    "            for j in range(len(B)):\n",
    "                b = B[j]\n",
    "                if a['sub'] == b['sub'] and a['rel'] == b['rel'] and a['obj'] == b['obj']:\n",
    "                    count += 1\n",
    "                    continue\n",
    "\n",
    "        return count\n",
    "\n",
    "\n",
    "    def count_unique(self, A, B):\n",
    "        \"\"\"\n",
    "        Функция, которая подсчитывает количество уникальных триплетов\n",
    "        \"\"\"\n",
    "\n",
    "        A = self.clean_list(A)\n",
    "        B = self.clean_list(B)\n",
    "\n",
    "        return len(A) + len(B) - self.count_matching(A, B)\n",
    "\n",
    "\n",
    "    def get_metric(self, expected: List[Set], predicted: List[Set]) -> float:\n",
    "\n",
    "        try:\n",
    "            X = expected\n",
    "            Y = predicted\n",
    "            return self.count_matching(X, Y) / self.count_unique(X, Y)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    def read_triples(self, path: str) -> List[List[Set]]:\n",
    "        with open(path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        result = []\n",
    "        for i_text in data:\n",
    "            i_text = json.loads(i_text)\n",
    "\n",
    "            triples = i_text['triples']\n",
    "            result.append(triples)\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def model_metric(self, expected_path: str, predicted_path: str) -> float:\n",
    "        \"\"\"Возвращает среднее значение меры Жаккара по двум наборам триплетов. \n",
    "        Триплеты считаются одинаковыми при полном совпадении\"\"\"\n",
    "\n",
    "        expected_list = self.read_triples(expected_path)\n",
    "        predicted_list = self.read_triples(predicted_path)\n",
    "        metric_arr = []\n",
    "\n",
    "        for expected, predicted in zip(expected_list, predicted_list):\n",
    "            metric = self.get_metric(expected, predicted)\n",
    "            metric_arr.append(metric)\n",
    "\n",
    "        return np.array(metric_arr).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.24699221514508135, 0.24309978768577492)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_metric = Jaccard_metric()\n",
    "jaccard_metric_base = jaccard_metric.model_metric(\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_gt.jsonl'),\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_base.jsonl'),\n",
    ")\n",
    "jaccard_metric_ft = jaccard_metric.model_metric(\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_gt.jsonl'),\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_ft.jsonl'),\n",
    ")\n",
    "jaccard_metric_ft_pp = jaccard_metric.model_metric(\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_gt.jsonl'),\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_ft_pp.jsonl'),\n",
    ")\n",
    "\n",
    "jaccard_metric_base, jaccard_metric_ft, jaccard_metric_ft_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_metrics(self, gold: Set, pred: Set) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate precision, recall and f1:\n",
    "            Precision is calculated as correct_triples/predicted_triples and\n",
    "            Recall as correct_triples/gold_triples\n",
    "            F1 as the harmonic mean of precision and recall.\n",
    "        :param gold: items in the gold standard\n",
    "        :param pred: items in the system prediction\n",
    "        :return:\n",
    "            p: float - precision\n",
    "            r: float - recall\n",
    "            f1: float - F1\n",
    "        \"\"\"\n",
    "        if len(pred) == 0:\n",
    "            return 0, 0, 0\n",
    "        p = len(gold.intersection(pred)) / len(pred)\n",
    "        r = len(gold.intersection(pred)) / len(gold)\n",
    "        if p + r > 0:\n",
    "            f1 = 2 * ((p * r) / (p + r))\n",
    "        else:\n",
    "            f1 = 0\n",
    "        return p, r, f1\n",
    "    \n",
    "\n",
    "    def normalize_triple(self, sub_label: str, rel_label: str, obj_label: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize triples for comparison in precision, recall calculations\n",
    "        :param sub_label: subject string\n",
    "        :param rel_label: relation string\n",
    "        :param obj_label: object string\n",
    "        :return: a normalized triple as a single concatenated string\n",
    "        \"\"\"\n",
    "        # remove spaces and underscores and make lower case\n",
    "        sub_label = re.sub(r\"(_|\\s+)\", '', sub_label).lower()\n",
    "        rel_label = re.sub(r\"(_|\\s+)\", '', rel_label).lower()\n",
    "        obj_label = re.sub(r\"(_|\\s+)\", '', obj_label).lower()\n",
    "        # concatenate them to a single string\n",
    "        tr_key = f\"{sub_label}{rel_label}{obj_label}\"\n",
    "        return tr_key\n",
    "    \n",
    "    def filter_triplets(self, file_path, pred=False) -> List[str]:\n",
    "        \"\"\"\n",
    "        Filter triplets from a file.\n",
    "        :param file_path: path to the file containing triplets\n",
    "        :param pred: flag indicating whether the file contains predictions or gold standard triplets\n",
    "        :return: filtered triplets as a list, or filtered triplets and model name as a tuple if pred=True\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = f.readlines()\n",
    "        filtered_triplets = []\n",
    "        ############################################\n",
    "        # здесь можно указать срез по данным, чтобы сравнивались фактически обработанные тексты\n",
    "        ############################################\n",
    "        for i_line in data:  #[:10]: # срез по данным. Кол-во n_run из тетрадки по генерации триплетов\n",
    "            json_data = json.loads(i_line)\n",
    "            for i_triple in json_data['triples']:\n",
    "                try:\n",
    "                    if i_triple['rel'] not in [\"cost\", \"main subject\", \"publication date\"]:\n",
    "                        if i_triple['obj'] != \"\":\n",
    "                            filtered_triplets.append(i_triple)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        if pred:\n",
    "            model = json_data.get('model', f\"{json_data.get('model1', '')}+{json_data.get('model2', '')}\")\n",
    "            return [filtered_triplets, model]\n",
    "        \n",
    "        return [filtered_triplets]\n",
    "\n",
    "\n",
    "    def evaluate(self, pred_file_path: str, gold_file_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate the performance of a prediction file against a gold standard file.\n",
    "        :param pred_file_path: path to the prediction file\n",
    "        :param gold_file_path: path to the gold standard file\n",
    "        :return: a dictionary containing the model name, precision, recall, and F1 score\n",
    "        \"\"\"\n",
    "        pred_temp = self.filter_triplets(pred_file_path, pred=True)\n",
    "        pred = pred_temp[0]\n",
    "        model = pred_temp[1]\n",
    "        gold = self.filter_triplets(gold_file_path)[0]\n",
    "        pred_ready = {self.normalize_triple(tr['sub'], tr['rel'], tr['obj']) for tr in pred}\n",
    "        gold_ready = {self.normalize_triple(tr['sub'], tr['rel'], tr['obj']) for tr in gold}\n",
    "        p, r, f1 = self.get_metrics(gold_ready, pred_ready)\n",
    "        return {\"model\": model, \"precision\": p, \"recall\": r, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'model': 'llama-2-7b.Q4_0.gguf+llama-2-7b.Q4_0.gguf',\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0},\n",
       " {'model': 'Llama-2-7b-m1ft-q4.gguf+Llama-2-7b-m2ft-q4.gguf',\n",
       "  'precision': 0.5555555555555556,\n",
       "  'recall': 0.28125,\n",
       "  'f1': 0.37344398340248963})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = GraphEvaluator()\n",
    "\n",
    "metrics_base = evaluator.evaluate(\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_base.jsonl'), \n",
    "    os.path.join('artifacts', DOMAIN, 'triples_gt.jsonl'))\n",
    "\n",
    "metrics_ft = evaluator.evaluate(\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_ft.jsonl'), \n",
    "    os.path.join('artifacts', DOMAIN, 'triples_gt.jsonl'))\n",
    "\n",
    "metrics_ft_pp = evaluator.evaluate(\n",
    "    os.path.join('artifacts', DOMAIN, 'triples_ft_pp.jsonl'), \n",
    "    os.path.join('artifacts', DOMAIN, 'triples_gt.jsonl'))\n",
    "\n",
    "metrics_base, metrics_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>knowknowledge_graph</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nature</td>\n",
       "      <td>ground truth</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nature</td>\n",
       "      <td>base</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nature</td>\n",
       "      <td>finetuned</td>\n",
       "      <td>0.246992</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.28125</td>\n",
       "      <td>0.373444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nature</td>\n",
       "      <td>finetuned_postprocessed</td>\n",
       "      <td>0.243100</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.26875</td>\n",
       "      <td>0.385650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain      knowknowledge_graph   jaccard  precision   recall        f1\n",
       "0  nature             ground truth  1.000000   1.000000  1.00000  1.000000\n",
       "1  nature                     base  0.000000   0.000000  0.00000  0.000000\n",
       "2  nature                finetuned  0.246992   0.555556  0.28125  0.373444\n",
       "3  nature  finetuned_postprocessed  0.243100   0.682540  0.26875  0.385650"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    'domain': [DOMAIN, DOMAIN, DOMAIN, DOMAIN],\n",
    "    'knowknowledge_graph': ['ground truth','base', 'finetuned', 'finetuned_postprocessed'],\n",
    "    'jaccard': [1.0, jaccard_metric_base, jaccard_metric_ft, jaccard_metric_ft_pp],\n",
    "    'precision': [1.0, metrics_base['precision'], metrics_ft['precision'], metrics_ft_pp['precision']],\n",
    "    'recall': [1.0, metrics_base['recall'], metrics_ft['recall'], metrics_ft_pp['recall']],\n",
    "    'f1': [1.0, metrics_base['f1'], metrics_ft['f1'], metrics_ft_pp['f1']],\n",
    "    })\n",
    "\n",
    "fn = os.path.join('artifacts', DOMAIN, 'kg_metrics.csv')\n",
    "metrics_df.to_csv(fn, index=False)\n",
    "\n",
    "pd.read_csv(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
